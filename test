class NpuMoEPrepareAndFinalize(FusedMoEPrepareAndFinalize):
    """
    NPU implementation of MoE prepare and finalize operations.
    Uses torch_npu.npu_moe_* ops.
    """

    def __init__(self, moe: FusedMoEConfig):
        super().__init__()
        self.moe = moe
        self.moe_all_to_all_group = get_ep_group().device_group
        self.moe_all_to_all_group_name = self.moe_all_to_all_group._get_backend(
                    torch.device(current_platform.device_type)).get_hccl_comm_name(
                    get_ep_group().rank_in_group)
        # self.moe_all_to_all_group_name = self.moe_all_to_all_group._get_backend(
        #             torch.device(current_platform.device_type)).get_hccl_comm_name(
        #             get_ep_group().rank_in_group)
        self.expand_idx = None
        self.ep_recv_counts = None
        self.tp_recv_counts = None


 def prepare(
        self,
        a1: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        num_experts: int,
        expert_map: torch.Tensor | None,
        apply_router_weight_on_input: bool,
        quant_config: FusedMoEQuantConfig,
    ) -> PrepareResultType:
        if quant_config.use_int8_w8a8:
            quant_mode = 2  # Dynamic quantization
        else:
            quant_mode = 0  # No quantization

        self.num_experts = num_experts

        kwargs = {
            "x": a1,
            "expert_ids": topk_ids,  # [n*topk]
            "expert_shard_type": 0,  # Set it to 0 for now
            "shared_expert_rank_num": 0,  # 32
            "moe_expert_num": self.num_experts, #ENABLE_OMNI_PLANNER, 0 redundancy 256, 1 redundancy expert 320
            "global_bs": 0,  # 0 Default (all); all tokens can be set
            "scales": None,  # Quantization coefficient
            "quant_mode": quant_mode,
            "group_ep": self.moe_all_to_all_group_name,  # Unlike torch, it is obtained by name.
            "ep_world_size": self.moe.ep_size,
            "ep_rank_id": self.moe.ep_rank,
            "x_active_mask": self._get_mc2_mask(topk_ids.shape[0]),
        }

        output = torch_npu.npu_moe_distribute_dispatch_v2(**kwargs)
        expand_x, dynamic_scale, expand_idx, expert_token_nums, ep_recv_counts, tp_recv_counts = output[0:6]
        expert_tokens_meta = ExpertTokensMetadata(
   
